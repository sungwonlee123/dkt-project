#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Assistment ìˆ˜í•™ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ (Model 1)
ìˆ˜í•™ ê°œë…ê°„ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ì—¬ ë°ì´í„°ì…‹ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import warnings
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
warnings.filterwarnings('ignore')

def extract_math_keywords(skill_name):
    """
    ìˆ˜í•™ ìŠ¤í‚¬ ì´ë¦„ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if pd.isna(skill_name):
        return []
    
    # ìˆ˜í•™ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤í•‘
    math_keywords = {
        # ê¸°ë³¸ ì—°ì‚°
        'addition': ['add', 'plus', 'sum', 'total'],
        'subtraction': ['subtract', 'minus', 'difference', 'take away'],
        'multiplication': ['multiply', 'times', 'product'],
        'division': ['divide', 'quotient', 'split'],
        
        # ë¶„ìˆ˜ ê´€ë ¨
        'fraction': ['fraction', 'numerator', 'denominator', 'ratio'],
        'decimal': ['decimal', 'point', 'tenth', 'hundredth'],
        'percent': ['percent', 'percentage', '%'],
        
        # ê¸°í•˜í•™
        'geometry': ['area', 'perimeter', 'volume', 'surface', 'angle', 'triangle', 'rectangle', 'circle'],
        'measurement': ['length', 'width', 'height', 'radius', 'diameter'],
        
        # ëŒ€ìˆ˜í•™
        'algebra': ['equation', 'variable', 'solve', 'expression', 'formula'],
        'graph': ['graph', 'plot', 'coordinate', 'axis'],
        
        # í†µê³„
        'statistics': ['mean', 'median', 'mode', 'average', 'range', 'probability'],
        'data': ['data', 'table', 'chart', 'diagram', 'venn'],
        
        # ê¸°íƒ€
        'number': ['integer', 'whole', 'positive', 'negative', 'prime', 'factor'],
        'pattern': ['pattern', 'sequence', 'rule']
    }
    
    skill_lower = skill_name.lower()
    extracted_keywords = []
    
    for category, keywords in math_keywords.items():
        for keyword in keywords:
            if keyword in skill_lower:
                extracted_keywords.append(category)
                break
    
    return extracted_keywords

def calculate_skill_similarity(skills_list):
    """
    ìˆ˜í•™ ìŠ¤í‚¬ë“¤ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    print("ğŸ” ìˆ˜í•™ ê°œë… ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
    
    # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
    skill_keywords = {}
    for skill in skills_list:
        if pd.notna(skill):
            skill_keywords[skill] = extract_math_keywords(skill)
    
    # 2. TF-IDF ë²¡í„°í™”ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ìƒì„±
    skill_texts = []
    for skill, keywords in skill_keywords.items():
        if keywords:
            skill_texts.append(' '.join(keywords))
        else:
            skill_texts.append(skill.lower())  # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
    
    # 3. TF-IDF ë²¡í„°í™”
    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(skill_texts)
    
    # 4. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarity_matrix = cosine_similarity(tfidf_matrix)
    
    # 5. ìœ ì‚¬ë„ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    similarity_df = pd.DataFrame(
        similarity_matrix,
        index=skills_list,
        columns=skills_list
    )
    
    return similarity_df, skill_keywords

def find_similar_skills(similarity_df, threshold=0.3):
    """
    ìœ ì‚¬ë„ê°€ ë†’ì€ ìŠ¤í‚¬ ìŒì„ ì°¾ìŠµë‹ˆë‹¤.
    """
    similar_pairs = []
    
    for i in range(len(similarity_df)):
        for j in range(i+1, len(similarity_df)):
            skill1 = similarity_df.index[i]
            skill2 = similarity_df.columns[j]
            similarity = similarity_df.iloc[i, j]
            
            if similarity > threshold:
                similar_pairs.append({
                    'skill1': skill1,
                    'skill2': skill2,
                    'similarity': similarity
                })
    
    return pd.DataFrame(similar_pairs).sort_values('similarity', ascending=False)

def preprocess_assistment_model1(input_file='skill_builder_model1.csv', output_file='skill_builder_model1_processed.csv'):
    """
    Assistment ë°ì´í„°ì…‹ì„ ìˆ˜í•™ ê°œë… ìœ ì‚¬ë„ì™€ í•¨ê»˜ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        input_file (str): ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ
        output_file (str): ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ
    """
    
    print("ğŸš€ Assistment ìˆ˜í•™ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì‹œì‘ (Model 1)")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("ğŸ“ ë°ì´í„° ë¡œë”© ì¤‘...")
    try:
        df = pd.read_csv(input_file, encoding='latin-1', low_memory=False)
    except:
        df = pd.read_csv(input_file, encoding='utf-8', low_memory=False)
    
    print(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {df.shape}")
    print(f"ì»¬ëŸ¼: {list(df.columns)}")
    
    # 2. ê¸°ë³¸ ì „ì²˜ë¦¬
    print("\nğŸ”§ ê¸°ë³¸ ì „ì²˜ë¦¬ ì¤‘...")
    
    # skill_nameì´ ìˆëŠ” ë°ì´í„°ë§Œ ì„ íƒ (NA ì œê±°)
    df_clean = df.dropna(subset=['skill_name'])
    print(f"skill_name NA ì œê±° í›„: {df_clean.shape} (ì œê±°ëœ í–‰: {len(df) - len(df_clean):,}ê°œ)")
    
    # ì¤‘ë³µ ì œê±°
    df_clean = df_clean.drop_duplicates()
    print(f"ì¤‘ë³µ ì œê±° í›„: {df_clean.shape} (ì œê±°ëœ í–‰: {len(df) - len(df_clean):,}ê°œ)")
    
    # 3. ìˆ˜í•™ ê°œë… ìœ ì‚¬ë„ ë¶„ì„
    print("\nğŸ“Š ìˆ˜í•™ ê°œë… ìœ ì‚¬ë„ ë¶„ì„ ì¤‘...")
    
    # ê³ ìœ í•œ ìŠ¤í‚¬ ëª©ë¡ ì¶”ì¶œ
    unique_skills = df_clean['skill_name'].dropna().unique()
    print(f"ê³ ìœ í•œ ìˆ˜í•™ ìŠ¤í‚¬ ìˆ˜: {len(unique_skills)}")
    
    # ìŠ¤í‚¬ë³„ ì¶œí˜„ ë¹ˆë„ í™•ì¸
    skill_counts = df_clean['skill_name'].value_counts()
    print(f"ìŠ¤í‚¬ë³„ ì¶œí˜„ ë¹ˆë„ í†µê³„:")
    print(f"  ìµœëŒ€: {skill_counts.max():,}íšŒ")
    print(f"  ìµœì†Œ: {skill_counts.min():,}íšŒ")
    print(f"  í‰ê· : {skill_counts.mean():.1f}íšŒ")
    
    # ì „ì²´ ìŠ¤í‚¬ë¡œ ë¶„ì„ (112ê°œ ìŠ¤í‚¬ ëª¨ë‘ í¬í•¨)
    print(f"ì „ì²´ {len(unique_skills)}ê°œ ìŠ¤í‚¬ë¡œ ìœ ì‚¬ë„ ë¶„ì„ ì‹œì‘...")
    
    # ìœ ì‚¬ë„ ê³„ì‚° (ì „ì²´ ìŠ¤í‚¬)
    similarity_df, skill_keywords = calculate_skill_similarity(unique_skills)
    
    # ìœ ì‚¬í•œ ìŠ¤í‚¬ ìŒ ì°¾ê¸°
    similar_pairs = find_similar_skills(similarity_df, threshold=0.2)
    
    print(f"\nğŸ” ìœ ì‚¬í•œ ìˆ˜í•™ ê°œë… ìŒ (ìœ ì‚¬ë„ > 0.2):")
    print("=" * 50)
    for _, row in similar_pairs.head(10).iterrows():
        print(f"  {row['skill1']} â†” {row['skill2']}: {row['similarity']:.3f}")
    
    # 4. ìŠ¤í‚¬ë³„ í†µê³„ ì •ë³´
    print(f"\nğŸ“ˆ ìŠ¤í‚¬ë³„ í†µê³„ ì •ë³´:")
    print("=" * 50)
    skill_stats = df_clean.groupby('skill_name').agg({
        'correct': ['mean', 'count'],
        'attempt_count': 'mean',
        'ms_first_response': 'mean'
    }).round(3)
    
    skill_stats.columns = ['accuracy', 'problem_count', 'avg_attempts', 'avg_response_time']
    skill_stats = skill_stats.sort_values('problem_count', ascending=False)
    
    print("ìƒìœ„ 10ê°œ ìŠ¤í‚¬:")
    print(skill_stats.head(10))
    
    # 5. ìœ ì‚¬ë„ ì •ë³´ë¥¼ ë°ì´í„°ì…‹ì— ì¶”ê°€
    print("\nğŸ”— ìœ ì‚¬ë„ ì •ë³´ë¥¼ ë°ì´í„°ì…‹ì— ì¶”ê°€ ì¤‘...")
    
    # 5.1 ê°€ì¥ ìœ ì‚¬í•œ ìŠ¤í‚¬ê³¼ ìœ ì‚¬ë„ ì ìˆ˜ ì¶”ê°€
    def get_most_similar_skill(skill_name, similarity_df, similar_pairs):
        """ê°€ì¥ ìœ ì‚¬í•œ ìŠ¤í‚¬ê³¼ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ë°˜í™˜"""
        if skill_name not in similarity_df.index:
            return None, 0.0
        
        # í•´ë‹¹ ìŠ¤í‚¬ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ìŠ¤í‚¬ ì°¾ê¸°
        skill_similarities = similarity_df.loc[skill_name].drop(skill_name)
        if len(skill_similarities) == 0:
            return None, 0.0
        
        most_similar_skill = skill_similarities.idxmax()
        max_similarity = skill_similarities.max()
        
        return most_similar_skill, max_similarity
    
    # 5.2 ìŠ¤í‚¬ ê·¸ë£¹ ìƒì„± (ìœ ì‚¬ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§)
    def create_skill_groups(similarity_df, threshold=0.3):
        """ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤í‚¬ ê·¸ë£¹ì„ ìƒì„±"""
        from sklearn.cluster import AgglomerativeClustering
        
        # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ê±°ë¦¬ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³€í™˜
        distance_matrix = 1 - similarity_df.values
        
        # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        clustering = AgglomerativeClustering(
            n_clusters=None, 
            distance_threshold=1-threshold,  # ìœ ì‚¬ë„ 0.3 ì´ìƒì´ë©´ ê°™ì€ ê·¸ë£¹
            metric='precomputed',
            linkage='average'
        )
        
        cluster_labels = clustering.fit_predict(distance_matrix)
        
        # ìŠ¤í‚¬ëª…ê³¼ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ë§¤í•‘
        skill_groups = {}
        for i, skill in enumerate(similarity_df.index):
            skill_groups[skill] = f"Group_{cluster_labels[i]}"
        
        return skill_groups
    
    # 5.3 ìœ ì‚¬ë„ ê¸°ë°˜ ë‚œì´ë„ ì¡°ì •
    def calculate_adjusted_difficulty(skill_name, similarity_df, skill_stats):
        """ìœ ì‚¬ ìŠ¤í‚¬ë“¤ì˜ í‰ê·  ë‚œì´ë„ë¥¼ ê³„ì‚°"""
        if skill_name not in similarity_df.index:
            return skill_stats.get(skill_name, 0.5)
        
        # ìœ ì‚¬ë„ 0.3 ì´ìƒì¸ ìŠ¤í‚¬ë“¤ ì°¾ê¸°
        similar_skills = similarity_df.loc[skill_name][similarity_df.loc[skill_name] >= 0.3]
        similar_skills = similar_skills.drop(skill_name)  # ìê¸° ìì‹  ì œì™¸
        
        if len(similar_skills) == 0:
            return skill_stats.get(skill_name, 0.5)
        
        # ìœ ì‚¬ ìŠ¤í‚¬ë“¤ì˜ í‰ê·  ì •ë‹µë¥  ê³„ì‚° (ì •ë‹µë¥ ì´ ë†’ì„ìˆ˜ë¡ ì‰¬ì›€)
        similar_accuracies = []
        for similar_skill in similar_skills.index:
            if similar_skill in skill_stats.index:
                similar_accuracies.append(skill_stats.loc[similar_skill, 'accuracy'])
        
        if len(similar_accuracies) == 0:
            return skill_stats.get(skill_name, 0.5)
        
        # ê°€ì¤‘í‰ê·  (ìœ ì‚¬ë„ê°€ ë†’ì„ìˆ˜ë¡ ë” í° ê°€ì¤‘ì¹˜)
        weights = similar_skills.values
        weighted_avg = np.average(similar_accuracies, weights=weights)
        
        return weighted_avg
    
    # ìœ ì‚¬ë„ ì •ë³´ ì¶”ê°€
    print("  - ê°€ì¥ ìœ ì‚¬í•œ ìŠ¤í‚¬ê³¼ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°...")
    df_clean['most_similar_skill'] = None
    df_clean['similarity_score'] = 0.0
    
    for skill in df_clean['skill_name'].unique():
        mask = df_clean['skill_name'] == skill
        most_similar, score = get_most_similar_skill(skill, similarity_df, similar_pairs)
        df_clean.loc[mask, 'most_similar_skill'] = most_similar
        df_clean.loc[mask, 'similarity_score'] = score
    
    # ìŠ¤í‚¬ ê·¸ë£¹ ìƒì„±
    print("  - ìŠ¤í‚¬ ê·¸ë£¹ ìƒì„±...")
    skill_groups = create_skill_groups(similarity_df, threshold=0.3)
    df_clean['skill_group'] = df_clean['skill_name'].map(skill_groups)
    
    # ì¡°ì •ëœ ë‚œì´ë„ ê³„ì‚°
    print("  - ìœ ì‚¬ë„ ê¸°ë°˜ ë‚œì´ë„ ì¡°ì •...")
    df_clean['adjusted_difficulty'] = df_clean['skill_name'].apply(
        lambda x: calculate_adjusted_difficulty(x, similarity_df, skill_stats)
    )
    
    # 6. ìµœì¢… ë°ì´í„° ì •ë¦¬
    print("\nğŸ“‹ ìµœì¢… ë°ì´í„° ì •ë¦¬ ì¤‘...")
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (ìœ ì‚¬ë„ ì •ë³´ í¬í•¨ + ë” ë§ì€ ì›ë³¸ ì •ë³´)
    final_columns = [
        # ê¸°ë³¸ ì •ë³´
        'order_id', 'assignment_id', 'user_id', 'assistment_id', 'problem_id', 
        'original', 'correct', 'attempt_count', 'ms_first_response', 
        'tutor_mode', 'answer_type', 'sequence_id', 'student_class_id', 
        'position', 'type', 'base_sequence_id', 'skill_id', 'skill_name', 
        'teacher_id', 'school_id', 'hint_count', 'hint_total', 'overlap_time', 
        'template_id', 'answer_id', 'answer_text', 'first_action', 
        'bottom_hint', 'opportunity', 'opportunity_original',
        # ì¶”ê°€ëœ ìœ ì‚¬ë„ ì •ë³´
        'most_similar_skill', 'similarity_score', 'skill_group', 'adjusted_difficulty'
    ]
    
    df_final = df_clean[final_columns].copy()
    
    # 7. í†µê³„ ì •ë³´ ì¶œë ¥
    print("\nğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼ í†µê³„:")
    print(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {df_final.shape}")
    print(f"í•™ìƒ ìˆ˜: {df_final['user_id'].nunique():,}")
    print(f"ë¬¸ì œ ìˆ˜: {df_final['problem_id'].nunique():,}")
    print(f"ìŠ¤í‚¬ ìˆ˜: {df_final['skill_name'].nunique():,}")
    
    print(f"\nğŸ”— ì¶”ê°€ëœ ìœ ì‚¬ë„ ì •ë³´:")
    print(f"ìŠ¤í‚¬ ê·¸ë£¹ ìˆ˜: {df_final['skill_group'].nunique()}")
    print(f"í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜: {df_final['similarity_score'].mean():.3f}")
    print(f"í‰ê·  ì¡°ì •ëœ ë‚œì´ë„: {df_final['adjusted_difficulty'].mean():.3f}")
    
    print(f"\nğŸ“ˆ ìŠ¤í‚¬ ê·¸ë£¹ ë¶„í¬:")
    print(df_final['skill_group'].value_counts().head(10))
    
    # 8. íŒŒì¼ ì €ì¥
    print(f"\nğŸ’¾ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì¤‘: {output_file}")
    df_final.to_csv(output_file, index=False, encoding='utf-8')
    
    # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ë„ ì €ì¥
    similarity_file = output_file.replace('.csv', '_similarity_matrix.csv')
    similarity_df.to_csv(similarity_file, encoding='utf-8')
    print(f"ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ì €ì¥: {similarity_file}")
    
    # ìœ ì‚¬í•œ ìŠ¤í‚¬ ìŒë„ ì €ì¥
    pairs_file = output_file.replace('.csv', '_similar_pairs.csv')
    similar_pairs.to_csv(pairs_file, index=False, encoding='utf-8')
    print(f"ìœ ì‚¬í•œ ìŠ¤í‚¬ ìŒ ì €ì¥: {pairs_file}")
    
    print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 60)
    
    return df_final, similarity_df, similar_pairs

if __name__ == "__main__":
    # ì „ì²˜ë¦¬ ì‹¤í–‰
    processed_data, similarity_matrix, similar_pairs = preprocess_assistment_model1()
    
    print("\nğŸ¯ ì „ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ:")
    print(processed_data.head())
    
    print("\nğŸ”— ì¶”ê°€ëœ ìœ ì‚¬ë„ ì •ë³´ ìƒ˜í”Œ:")
    print("=" * 50)
    sample_data = processed_data[['skill_name', 'most_similar_skill', 'similarity_score', 'skill_group', 'adjusted_difficulty']].head(10)
    print(sample_data)
    
    print("\nğŸ” ìˆ˜í•™ ê°œë… ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼:")
    print("=" * 50)
    print(f"ì´ {len(similar_pairs)}ê°œì˜ ìœ ì‚¬í•œ ìŠ¤í‚¬ ìŒ ë°œê²¬!")
    print("ìƒìœ„ 15ê°œ ìœ ì‚¬í•œ ìŠ¤í‚¬ ìŒ:")
    print(similar_pairs.head(15))
    
    print(f"\nğŸ“Š ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ í¬ê¸°: {similarity_matrix.shape}")
    print("ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒ˜í”Œ (ìƒìœ„ 10x10):")
    print(similarity_matrix.iloc[:10, :10].round(3))
    
    print(f"\nğŸ¯ ê°€ì¥ ìœ ì‚¬í•œ ìˆ˜í•™ ê°œë… ìŒ TOP 10:")
    print("=" * 60)
    for i, (_, row) in enumerate(similar_pairs.head(10).iterrows(), 1):
        print(f"{i:2d}. {row['skill1']} â†” {row['skill2']}: {row['similarity']:.3f}")
